{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb96153-5874-4d41-aaa1-9875b143a5c0",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8339293-403f-40a1-afca-c516b5b2b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6edfdc-026f-4ebe-a546-f569a1992caa",
   "metadata": {},
   "source": [
    "## Initializations and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b459f1f8-96a9-440a-bdba-8f828a00ab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcasarulez\u001b[0m (\u001b[33mHarish-Personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mithun/Harish-SparkIntern/wandb/run-20250225_142733-l4d42tji</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Harish-Personal/AT-DQN/runs/l4d42tji' target=\"_blank\">Pong_ATDQN_PyTorch</a></strong> to <a href='https://wandb.ai/Harish-Personal/AT-DQN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Harish-Personal/AT-DQN' target=\"_blank\">https://wandb.ai/Harish-Personal/AT-DQN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Harish-Personal/AT-DQN/runs/l4d42tji' target=\"_blank\">https://wandb.ai/Harish-Personal/AT-DQN/runs/l4d42tji</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/Harish-Personal/AT-DQN/runs/l4d42tji?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0fc2d17820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"AT-DQN\", name=\"Pong_ATDQN_PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8aedbd-58d7-4f59-8115-4e976e93d33b",
   "metadata": {},
   "source": [
    "## Preprocessing of ATARI frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880c28f5-6b4a-4d10-b716-92ae6ecbf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    frame = torch.tensor(frame, dtype=torch.float32).mean(dim=-1)  \n",
    "    frame = torch.nn.functional.interpolate(frame.unsqueeze(0).unsqueeze(0), size=(84, 84)).squeeze()\n",
    "    return frame.numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913cd28-a1a0-4a0c-96cf-3fa686ae4e04",
   "metadata": {},
   "source": [
    "## Building the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09ddc04-e685-44dc-8989-901ee0f1741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network using PyTorch\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_shape, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a2dfc-7ae8-4fec-a351-5401b5143435",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe33235c-4e7f-42c2-b869-51562b2349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer for Experience Replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbe7c2-0b7c-4c3a-8b51-6ac9ee26ce70",
   "metadata": {},
   "source": [
    "## Main AT-DQN Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60c1cd4-e303-4545-9139-70177cbf4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention-based DQN Agent\n",
    "class ATDQNAgent:\n",
    "    def __init__(self, action_size, state_shape, tau=0.2, beta_start=0.4, beta_end=1.0, T=1000, device=\"cuda\"):\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.q_network = QNetwork(state_shape, action_size).to(device)\n",
    "        self.target_network = QNetwork(state_shape, action_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.00025)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.gamma = 0.99  \n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.alpha = defaultdict(lambda: 1.0)  \n",
    "        self.td_errors = defaultdict(list)\n",
    "\n",
    "        # Exploration threshold\n",
    "        self.tau = tau  \n",
    "\n",
    "        # Beta Annealing\n",
    "        self.beta = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.delta_beta = (beta_end - beta_start) / T  \n",
    "\n",
    "    def get_attention(self, state):\n",
    "        return self.alpha[tuple(state.flatten())]\n",
    "\n",
    "    def update_attention(self, state, td_error):\n",
    "        state_key = tuple(state.flatten())\n",
    "        self.td_errors[state_key].append(abs(td_error))\n",
    "        self.alpha[state_key] = np.mean(self.td_errors[state_key])\n",
    "\n",
    "    def normalize_attention(self):\n",
    "        max_alpha = max(self.alpha.values(), default=1)\n",
    "        for state_key in self.alpha:\n",
    "            self.alpha[state_key] /= max_alpha  \n",
    "\n",
    "    def compute_importance_weight(self, state, N):\n",
    "        alpha_s = self.alpha[tuple(state.flatten())]\n",
    "        return (1 / (N * alpha_s)) ** self.beta\n",
    "\n",
    "    def act(self, state):\n",
    "        sigma = self.get_attention(state)\n",
    "        if sigma > self.tau:\n",
    "            return np.random.choice(self.action_size)  \n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "            \n",
    "        if self.replay_buffer.size() < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.replay_buffer.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).to(self.device)\n",
    "        dones = torch.FloatTensor(np.array(dones).astype(int)).to(self.device)\n",
    "\n",
    "        target_q_values = self.target_network(next_states).detach()\n",
    "        max_next_q = target_q_values.max(dim=1)[0]\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "\n",
    "        q_values = self.q_network(states)\n",
    "        q_values = q_values.gather(1, torch.LongTensor(actions).unsqueeze(1).to(self.device)).squeeze()\n",
    "\n",
    "        td_errors = targets - q_values\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            self.update_attention(states[i].cpu().numpy(), td_errors[i].item())\n",
    "\n",
    "        # Importance sampling correction\n",
    "        N = len(self.replay_buffer.buffer)\n",
    "        importance_weights = torch.FloatTensor(\n",
    "            [self.compute_importance_weight(states[i].cpu().numpy(), N) for i in range(batch_size)]\n",
    "        ).to(self.device)\n",
    "\n",
    "        importance_weights /= importance_weights.max()  \n",
    "\n",
    "        loss = torch.mean(importance_weights * (td_errors ** 2))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def anneal_beta(self):\n",
    "        self.beta = min(self.beta + self.delta_beta, self.beta_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f861d-930a-4e6c-9f96-2803fa72cc38",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bd4f7-cd8e-44bb-b4e6-c401bcd9cf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created successfully!\n",
      "Environment created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodic Progress:   0%|                          | 0/1000 [00:00<?, ?episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/miniconda3/envs/AT-DQN/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "total_cumulative_reward = 0  # Initialize cumulative reward\n",
    "td_error=[]\n",
    "\n",
    "# Step 1: Check if environment is created\n",
    "try:\n",
    "    env = gym.make('ALE/Pong-v5')\n",
    "    print(\"Environment created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in creating environment: {e}\")\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "state_shape = (4, 84, 84)  # 4 stacked frames, 84x84 resolution\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = ATDQNAgent(action_size, state_shape, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"AT-DQN\", config={\"episodes\": 1000, \"batch_size\": 32})\n",
    "\n",
    "# Step 1: Check if environment is created\n",
    "try:\n",
    "    env = gym.make('ALE/Pong-v5')\n",
    "    print(\"Environment created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in creating environment: {e}\")\n",
    "    exit()\n",
    "\n",
    "obs = env.reset()\n",
    "state_shape = (4, 84, 84)  # 4 stacked frames, 84x84 resolution\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize agent\n",
    "agent = ATDQNAgent(action_size, state_shape, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "reward_memory = deque(maxlen=100)  # Stores the last 100 rewards\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Episodic Progress\", unit=\"episode\"):\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} started...\")\n",
    "\n",
    "    state, info = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state_stack = np.stack([state] * 4, axis=0)  # Expected: (4, 84, 84)\n",
    "    total_reward = 0\n",
    "    loss = 0.0\n",
    "    beta = agent.beta  # Assuming ATDQNAgent has a beta parameter\n",
    "    td_error=[]\n",
    "\n",
    "    for step in range(10000):\n",
    "        action = agent.act(state_stack)\n",
    "        try:\n",
    "            next_frame, reward, done, _, _ = env.step(action)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in env.step(): {e}\")\n",
    "            break\n",
    "\n",
    "        next_frame = preprocess_frame(next_frame)\n",
    "        next_state_stack = np.concatenate((state_stack[1:], np.expand_dims(next_frame, axis=0)), axis=0)\n",
    "\n",
    "        agent.replay_buffer.add((state_stack, action, reward, next_state_stack, done))\n",
    "        loss = agent.train(batch_size)\n",
    "\n",
    "        state_stack = next_state_stack\n",
    "        total_reward += reward\n",
    "        td_error.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode completed!\")\n",
    "            break\n",
    "        \n",
    "    total_cumulative_reward += total_reward\n",
    "    reward_memory.append(total_reward)  # Store reward for averaging\n",
    "    \n",
    "    mean_td_error = np.mean([x for x in td_error if x is not None]) if td_error else 0.0\n",
    "    # Log episodic metrics to wandb\n",
    "    wandb.log({\n",
    "        \"Beta per episode\": beta,\n",
    "        \"Mean TD error per episode\": mean_td_error,\n",
    "        \"Reward per episode\": total_reward,\n",
    "        \"Total Reward vs Mean TD Error\": wandb.Table(data=[[total_reward, mean_td_error]], columns=[\"Total Reward\", \"Mean TD Error\"])\n",
    "    })\n",
    "\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward}, Loss = {loss:.4f}, Beta = {beta:.4f}\")\n",
    "\n",
    "    agent.anneal_beta()\n",
    "\n",
    "    # Update target network every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        agent.update_target_network()\n",
    "        print(f\"âœ… Updated target network at episode {episode + 1}\")\n",
    "\n",
    "    # Logs every 100 episodes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        wandb.log({\n",
    "            \"Beta per 100 episodes\": beta,\n",
    "            \"Reward per 100 episodes\": total_reward,\n",
    "            \"Average reward over last 100 episodes\": np.mean(reward_memory)\n",
    "        })\n",
    "\n",
    "env.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bc267-f19d-4deb-8def-7d6e99148e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AT-DQN",
   "language": "python",
   "name": "at-dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
