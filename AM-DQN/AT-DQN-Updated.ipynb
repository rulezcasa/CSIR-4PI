{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb96153-5874-4d41-aaa1-9875b143a5c0",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8339293-403f-40a1-afca-c516b5b2b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:45:38.849741: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-13 11:45:38.879614: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-13 11:45:38.879644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-13 11:45:38.880493: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 11:45:38.885689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 11:45:39.376834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6edfdc-026f-4ebe-a546-f569a1992caa",
   "metadata": {},
   "source": [
    "## Initializations and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b459f1f8-96a9-440a-bdba-8f828a00ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau=0.2 \n",
    "# lambda_=1.0\n",
    "# beta_start=0.4\n",
    "# beta_end=1.0\n",
    "# T=1000000\n",
    "# gamma=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8aedbd-58d7-4f59-8115-4e976e93d33b",
   "metadata": {},
   "source": [
    "## Preprocessing of ATARI frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880c28f5-6b4a-4d10-b716-92ae6ecbf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Grayscale\n",
    "    frame = cv2.resize(frame, (84, 84))  # Resize to 84x84\n",
    "    frame = frame / 255.0  # Normalize\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913cd28-a1a0-4a0c-96cf-3fa686ae4e04",
   "metadata": {},
   "source": [
    "## Building the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09ddc04-e685-44dc-8989-901ee0f1741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_network(input_shape, action_size):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (8, 8), strides=4, activation='relu', input_shape=input_shape),\n",
    "        keras.layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
    "        keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00025), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a2dfc-7ae8-4fec-a351-5401b5143435",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe33235c-4e7f-42c2-b869-51562b2349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbe7c2-0b7c-4c3a-8b51-6ac9ee26ce70",
   "metadata": {},
   "source": [
    "## Main AT-DQN Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60c1cd4-e303-4545-9139-70177cbf4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATDQNAgent:\n",
    "    def __init__(self, action_size, state_shape, tau=0.2, lambda_=1.0, beta_start=0.4, beta_end=1.0, T=1000000):\n",
    "        self.action_size = action_size\n",
    "        self.q_network = build_q_network(state_shape, action_size)\n",
    "        self.target_network = build_q_network(state_shape, action_size)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.gamma = 0.99  \n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.alpha = defaultdict(lambda: 1.0)  # Default attention weight = 1\n",
    "        self.td_errors = defaultdict(list)\n",
    "\n",
    "        # Exploration and importance weighting\n",
    "        self.tau = tau  \n",
    "        self.lambda_ = lambda_  \n",
    "\n",
    "        # Beta Annealing\n",
    "        self.beta = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.delta_beta = (beta_end - beta_start) / T  \n",
    "\n",
    "    def get_attention(self, state):\n",
    "        return self.alpha[tuple(state.flatten())]\n",
    "\n",
    "    def update_attention(self, state, td_error):\n",
    "        state_key = tuple(state.flatten())\n",
    "        self.td_errors[state_key].append(abs(td_error))\n",
    "        self.alpha[state_key] = np.mean(self.td_errors[state_key])\n",
    "\n",
    "    def normalize_attention(self):\n",
    "        max_alpha = max(self.alpha.values(), default=1)\n",
    "        for state_key in self.alpha:\n",
    "            self.alpha[state_key] /= max_alpha  \n",
    "\n",
    "    def compute_importance_weight(self, state, N):\n",
    "        alpha_s = self.alpha[tuple(state.flatten())]\n",
    "        return (1 / (N * alpha_s)) ** self.beta\n",
    "\n",
    "    def act(self, state):\n",
    "        sigma = self.get_attention(state) * self.lambda_\n",
    "        if sigma > self.tau:\n",
    "            return np.random.choice(self.action_size)  \n",
    "        else:\n",
    "            q_values = self.q_network.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "            return np.argmax(q_values[0])  \n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        if self.replay_buffer.size() < batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        target_q_values = self.target_network.predict(next_states, verbose=0)\n",
    "        max_next_q = np.max(target_q_values, axis=1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "\n",
    "        q_values = self.q_network.predict(states, verbose=0)\n",
    "        td_errors = targets - q_values[np.arange(batch_size), actions]\n",
    "\n",
    "        # Update attention weights\n",
    "        for i in range(batch_size):\n",
    "            self.update_attention(states[i], td_errors[i])\n",
    "\n",
    "        # Importance sampling correction\n",
    "        N = len(self.replay_buffer.buffer)\n",
    "        importance_weights = np.array([self.compute_importance_weight(states[i], N) for i in range(batch_size)])\n",
    "        importance_weights /= importance_weights.max()  # Normalize\n",
    "\n",
    "        # Weighted TD loss\n",
    "        loss = np.mean(importance_weights * (td_errors ** 2))\n",
    "\n",
    "        # Backpropagate loss\n",
    "        self.q_network.fit(states, q_values, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def anneal_beta(self):\n",
    "        self.beta = min(self.beta + self.delta_beta, self.beta_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f861d-930a-4e6c-9f96-2803fa72cc38",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5bd4f7-cd8e-44bb-b4e6-c401bcd9cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcasarulez\u001b[0m (\u001b[33mHarish-Personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mithun/Harish-SparkIntern/wandb/run-20250213_114546-estt3fbe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Harish-Personal/AT-DQN/runs/estt3fbe' target=\"_blank\">Pong_ATDQN</a></strong> to <a href='https://wandb.ai/Harish-Personal/AT-DQN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Harish-Personal/AT-DQN' target=\"_blank\">https://wandb.ai/Harish-Personal/AT-DQN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Harish-Personal/AT-DQN/runs/estt3fbe' target=\"_blank\">https://wandb.ai/Harish-Personal/AT-DQN/runs/estt3fbe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "2025-02-13 11:45:47.512335: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/mithun/miniconda3/envs/AT-DQN/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "Training Progress:   0%|                          | 0/1000 [01:04<?, ?episode/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m td_error \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd((state_stack, action, reward, next_state_stack, done))\n\u001b[1;32m     36\u001b[0m episode_td_errors\u001b[38;5;241m.\u001b[39mappend(td_error)\n\u001b[0;32m---> 38\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m state_stack \u001b[38;5;241m=\u001b[39m next_state_stack\n\u001b[1;32m     40\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m, in \u001b[0;36mATDQNAgent.train\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Update attention weights\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd_errors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Importance sampling correction\u001b[39;00m\n\u001b[1;32m     67\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mbuffer)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"AT-DQN\", name=\"Pong_ATDQN\")\n",
    "\n",
    "env = gym.make('ALE/Pong-v5')\n",
    "state_shape = (84, 84, 4)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = ATDQNAgent(action_size, state_shape)\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "wandb.config.update({\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"batch_size\": batch_size\n",
    "})\n",
    "\n",
    "# Use tqdm to track episode progress\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Progress\", unit=\"episode\"):\n",
    "    state = preprocess_frame(env.reset()[0])\n",
    "    state_stack = np.stack([state] * 4, axis=-1)\n",
    "\n",
    "    total_reward = 0\n",
    "    episode_td_errors = []\n",
    "\n",
    "    for step in range(10000):\n",
    "        action = agent.act(state_stack)\n",
    "        next_frame, reward, done, _, _ = env.step(action)\n",
    "        next_state = preprocess_frame(next_frame)\n",
    "\n",
    "        next_state_stack = np.append(state_stack[:, :, 1:], np.expand_dims(next_state, axis=-1), axis=-1)\n",
    "\n",
    "        td_error = agent.replay_buffer.add((state_stack, action, reward, next_state_stack, done))\n",
    "        episode_td_errors.append(td_error)\n",
    "\n",
    "        agent.train(batch_size)\n",
    "        state_stack = next_state_stack\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.update_target_network()\n",
    "    agent.anneal_beta()\n",
    "    agent.normalize_attention()\n",
    "\n",
    "    # Log episode metrics\n",
    "    wandb.log({\n",
    "        \"Episode\": episode + 1,\n",
    "        \"Total Reward\": total_reward,\n",
    "        \"Mean TD Error\": np.mean(episode_td_errors),\n",
    "        \"Beta\": agent.beta\n",
    "    })\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward}, Î² = {agent.beta:.4f}\")\n",
    "\n",
    "env.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d411b0-b074-4e40-bc25-3c16a34970ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AT-DQN",
   "language": "python",
   "name": "at-dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
