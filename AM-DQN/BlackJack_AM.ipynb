{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103f8c14-fb82-4c8b-98f0-bb8698dc24da",
   "metadata": {},
   "source": [
    "## Importing depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef06e65b-1822-4510-b513-afdb9028f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv\n",
    ")\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ale_py\n",
    "from collections import deque\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d844388-1fd6-48a5-b69b-aeedab64a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6999b2-1289-4f98-9cf2-5c2bb5cd4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import GrayscaleObservation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317b919-22f5-4c94-9747-02e7458d8073",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1cd640-06c1-444c-b76a-f50e46ce3af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = \"ALE/Breakout-v5\"      # The environment to train on\n",
    "seed = 1                               # Random seed for reproducibility\n",
    "total_timesteps = 10000000             # Total timesteps to run the training\n",
    "learning_rate = 1e-4                   # Learning rate for the optimizer\n",
    "num_envs = 1                           # Number of parallel environments\n",
    "buffer_size = 1000000                  # Size of the replay buffer\n",
    "gamma = 0.99                           # Discount factor\n",
    "tau = 1.0                              # Target network update rate\n",
    "target_network_frequency = 1000        # Timesteps to update target network\n",
    "batch_size = 32                        # Batch size for training\n",
    "start_e = 1                            # Starting epsilon for exploration\n",
    "end_e = 0.01                           # Ending epsilon for exploration\n",
    "learning_starts = 80000                # Timesteps before starting training\n",
    "train_frequency = 4                    # Frequency of training\n",
    "exploration_fraction = 0.1  # Example value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f042b49-711a-4640-9201-a3e34f81af1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Environment Setup ---- #\n",
    "def make_env(env_id, seed, idx, capture_video=False, run_name=\"\"):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, 4)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178303e7-fbcb-4b52-8fda-5dd64efd35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Q-Network with Attention ---- #\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054c6a07-1c86-4a07-a872-d98290679d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetwork(QNetwork):\n",
    "    def __init__(self, env):\n",
    "        super(TargetNetwork, self).__init__(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907c3083-a1cf-41c2-8335-57319609c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)  # Remove the oldest experience if the buffer is full\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfaff17-e8dd-4f46-814f-397417f2c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDQN:\n",
    "    def __init__(self, env, q_network, target_network, learning_rate, gamma, tau, exploration_fraction):\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "        self.target_network = target_network\n",
    "        self.optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.attention_weights = {}  # Dict to store attention weights for states\n",
    "        self.td_error_history = {}   # Dict to store TD error history for states per episode\n",
    "\n",
    "    def select_action(self, state, epsilon=0.0):\n",
    "        # Exploration based on attention weight\n",
    "        sigma = self.attention_weights.get(state, 1.0) * self.exploration_fraction\n",
    "        if random.random() < sigma:\n",
    "            return self.env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            # Exploit: choose action with max Q-value\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def update_attention_weights(self, state, td_error):\n",
    "        # Accumulate TD error history per state for the episode\n",
    "        if state not in self.td_error_history:\n",
    "            self.td_error_history[state] = []\n",
    "        self.td_error_history[state].append(abs(td_error))\n",
    "\n",
    "        # Update attention weights based on cumulative TD error history\n",
    "        cumulative_td_error = np.mean(self.td_error_history[state])  # Average TD error for the state\n",
    "        self.attention_weights[state] = cumulative_td_error\n",
    "\n",
    "    def train(self, total_timesteps, train_frequency, learning_starts, target_network_frequency):\n",
    "        start_time = time.time()\n",
    "        obs, _ = self.env.reset(seed=seed)\n",
    "\n",
    "        # ---- Training Loop ---- #\n",
    "        for global_step in range(total_timesteps):\n",
    "            state = obs\n",
    "            episodic_return = 0\n",
    "            done = False\n",
    "\n",
    "            # Reset TD error history for new episode\n",
    "            episode_td_errors = []\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "                # Compute TD error for this step\n",
    "                with torch.no_grad():\n",
    "                    q_values_next = self.target_network(torch.tensor(next_state, dtype=torch.float32).to(device))\n",
    "                    target_value = reward + self.gamma * q_values_next.max(dim=1)[0]\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "                q_value = self.q_network(state_tensor)[0, action]\n",
    "                td_error = target_value - q_value.item()\n",
    "\n",
    "                # Accumulate TD error history for this episode\n",
    "                episode_td_errors.append(td_error)\n",
    "\n",
    "                # Store experience in buffer (optional)\n",
    "                # Replay buffer can be implemented here if needed\n",
    "\n",
    "                # Update Q-network\n",
    "                loss = F.mse_loss(torch.tensor(target_value), torch.tensor(q_value))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                state = next_state\n",
    "                episodic_return += reward\n",
    "\n",
    "            # After the episode, update attention weights for all states encountered in the episode\n",
    "            for state, td_error in zip([state] * len(episode_td_errors), episode_td_errors):\n",
    "                self.update_attention_weights(state, td_error)\n",
    "\n",
    "            # Update target network\n",
    "            if global_step % target_network_frequency == 0:\n",
    "                for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "                    target_param.data.copy_(\n",
    "                        self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                    )\n",
    "\n",
    "            # Print and log metrics, etc.\n",
    "            if global_step % train_frequency == 0:\n",
    "                print(f\"Step {global_step}, Episodic Return: {episodic_return}\")\n",
    "                # Add any other logging (e.g., WandB, TensorBoard) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "125bf2cb-96bf-43f5-ba6f-8073a208864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcasarulez\u001b[0m (\u001b[33mHarish-Personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mithun/Harish-SparkIntern/wandb/run-20250206_124959-ojkwzxn8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Harish-Personal/ADQN-RL/runs/ojkwzxn8' target=\"_blank\">ADQN_Breakout</a></strong> to <a href='https://wandb.ai/Harish-Personal/ADQN-RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Harish-Personal/ADQN-RL' target=\"_blank\">https://wandb.ai/Harish-Personal/ADQN-RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Harish-Personal/ADQN-RL/runs/ojkwzxn8' target=\"_blank\">https://wandb.ai/Harish-Personal/ADQN-RL/runs/ojkwzxn8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NamespaceNotFound",
     "evalue": "Namespace ALE not found. Have you installed the proper package for ALE?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNamespaceNotFound\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m      8\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      9\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADQN-RL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     }\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ---- Environment Setup ---- #\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m envs \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncVectorEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# ---- Initialize Q-Network and Target Network ---- #\u001b[39;00m\n\u001b[1;32m     32\u001b[0m q_network \u001b[38;5;241m=\u001b[39m QNetwork()  \u001b[38;5;66;03m# Initialize the Q-network (ensure this is defined correctly)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ATARI-HARISH/lib/python3.13/site-packages/gym/vector/sync_vector_env.py:52\u001b[0m, in \u001b[0;36mSyncVectorEnv.__init__\u001b[0;34m(self, env_fns, observation_space, action_space, copy)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [\u001b[43menv_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mmake_env.<locals>.thunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mRecordVideo(env, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mRecordEpisodeStatistics(env)\n\u001b[1;32m     11\u001b[0m env \u001b[38;5;241m=\u001b[39m NoopResetEnv(env, noop_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ATARI-HARISH/lib/python3.13/site-packages/gym/envs/registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/ATARI-HARISH/lib/python3.13/site-packages/gym/envs/registration.py:219\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ATARI-HARISH/lib/python3.13/site-packages/gym/envs/registration.py:187\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_name_exists\u001b[39m(ns: Optional[\u001b[38;5;28mstr\u001b[39m], name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if an env exists in a namespace. If it doesn't, print a helpful error message.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[43m_check_namespace_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     names \u001b[38;5;241m=\u001b[39m {spec_\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m spec_ \u001b[38;5;129;01min\u001b[39;00m registry\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m spec_\u001b[38;5;241m.\u001b[39mnamespace \u001b[38;5;241m==\u001b[39m ns}\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n",
      "File \u001b[0;32m~/miniconda3/envs/ATARI-HARISH/lib/python3.13/site-packages/gym/envs/registration.py:182\u001b[0m, in \u001b[0;36m_check_namespace_exists\u001b[0;34m(ns)\u001b[0m\n\u001b[1;32m    173\u001b[0m suggestion \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    174\u001b[0m     difflib\u001b[38;5;241m.\u001b[39mget_close_matches(ns, namespaces, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespaces) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    176\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m suggestion\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHave you installed the proper package for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNamespaceNotFound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNamespaceNotFound\u001b[0m: Namespace ALE not found. Have you installed the proper package for ALE?"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import gym\n",
    "\n",
    "# ---- WandB Setup ---- #\n",
    "run_name = \"ADQN_Breakout\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"ADQN-RL\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"env_id\": env_id,\n",
    "        \"seed\": seed,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"tau\": tau,\n",
    "        \"train_frequency\": train_frequency,\n",
    "        \"start_e\": start_e,\n",
    "        \"end_e\": end_e,\n",
    "        \"total_timesteps\": total_timesteps,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ---- Environment Setup ---- #\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, seed + i, i, capture_video=False, run_name=run_name) for i in range(num_envs)]\n",
    ")\n",
    "\n",
    "# ---- Initialize Q-Network and Target Network ---- #\n",
    "q_network = QNetwork()  # Initialize the Q-network (ensure this is defined correctly)\n",
    "target_network = TargetNetwork()  # Initialize the target network (ensure this is defined correctly)\n",
    "\n",
    "# ---- Initialize AttentionDQN Agent ---- #\n",
    "adqn = AttentionDQN(\n",
    "    envs=envs,\n",
    "    q_network=q_network,\n",
    "    target_network=target_network,\n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma,\n",
    "    tau=tau,\n",
    "    exploration_fraction=exploration_fraction\n",
    ")\n",
    "\n",
    "# ---- Training ---- #\n",
    "adqn.train()\n",
    "\n",
    "# ---- Save Model ---- #\n",
    "model_path = f\"runs/{run_name}/adqn_model.pth\"\n",
    "torch.save(adqn.q_network.state_dict(), model_path)\n",
    "wandb.save(model_path)\n",
    "print(f\"Model saved to {model_path} and uploaded to WandB\")\n",
    "\n",
    "envs.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfbd77-87bd-4b94-9fb1-3adf0e195f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATARI-HARISH",
   "language": "python",
   "name": "atari-harish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
