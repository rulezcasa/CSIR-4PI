{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bc7659-7cb4-4ead-95d2-d229bd266181",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4ba00b6-b97d-4eb7-9b50-265de254003e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"algor.png\" alt=\"Description\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb96153-5874-4d41-aaa1-9875b143a5c0",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8339293-403f-40a1-afca-c516b5b2b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6edfdc-026f-4ebe-a546-f569a1992caa",
   "metadata": {},
   "source": [
    "## Initializations and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459f1f8-96a9-440a-bdba-8f828a00ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau=0.2 \n",
    "# lambda_=1.0\n",
    "# beta_start=0.4\n",
    "# beta_end=1.0\n",
    "# T=1000000\n",
    "# gamma=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8aedbd-58d7-4f59-8115-4e976e93d33b",
   "metadata": {},
   "source": [
    "## Preprocessing of ATARI frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c28f5-6b4a-4d10-b716-92ae6ecbf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Grayscale\n",
    "    frame = cv2.resize(frame, (84, 84))  # Resize to 84x84\n",
    "    frame = frame / 255.0  # Normalize\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913cd28-a1a0-4a0c-96cf-3fa686ae4e04",
   "metadata": {},
   "source": [
    "## Building the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ddc04-e685-44dc-8989-901ee0f1741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_network(input_shape, action_size):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (8, 8), strides=4, activation='relu', input_shape=input_shape),\n",
    "        keras.layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
    "        keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00025), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a2dfc-7ae8-4fec-a351-5401b5143435",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33235c-4e7f-42c2-b869-51562b2349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbe7c2-0b7c-4c3a-8b51-6ac9ee26ce70",
   "metadata": {},
   "source": [
    "## Main AT-DQN Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c1cd4-e303-4545-9139-70177cbf4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATDQNAgent:\n",
    "    def __init__(self, action_size, state_shape, tau=0.2, lambda_=1.0, beta_start=0.4, beta_end=1.0, T=1000000):\n",
    "        self.action_size = action_size\n",
    "        self.q_network = build_q_network(state_shape, action_size)\n",
    "        self.target_network = build_q_network(state_shape, action_size)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.alpha = {}  # Attention weights per state\n",
    "        self.td_errors = {}  # TD error history\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.tau = tau  # Threshold\n",
    "        self.lambda_ = lambda_  # Score adjustment factor\n",
    "\n",
    "        # Beta Annealing\n",
    "        self.beta = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.delta_beta = (beta_end - beta_start) / T  # Annealing rate\n",
    "\n",
    "    #Returns attention weight for a state or initializes with 1 if not present\n",
    "    def get_attention(self, state):\n",
    "        state_key = tuple(state.flatten())\n",
    "        return self.alpha.get(state_key, 1.0)\n",
    "\n",
    "    #Updates attention weights based on cumulative TD error over the episode\n",
    "    def update_attention(self, state, td_error):\n",
    "        state_key = tuple(state.flatten())\n",
    "        if state_key not in self.td_errors:\n",
    "            self.td_errors[state_key] = []\n",
    "        self.td_errors[state_key].append(abs(td_error))\n",
    "\n",
    "        self.alpha[state_key] = np.mean(self.td_errors[state_key])\n",
    "\n",
    "    #Normalization using Beta-Annealing\n",
    "    def normalize_attention(self):\n",
    "        max_alpha = max(self.alpha.values(), default=1)\n",
    "        for state_key in self.alpha:\n",
    "            self.alpha[state_key] /= max_alpha\n",
    "\n",
    "    #Select action based on attention-weighted exploration strategy\n",
    "    def act(self, state):\n",
    "        state_key = tuple(state.flatten())\n",
    "        sigma = self.get_attention(state) * self.lambda_\n",
    "\n",
    "        if sigma > self.tau:\n",
    "            return np.random.choice(self.action_size)  # Explore\n",
    "        else:\n",
    "            q_values = self.q_network.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "            return np.argmax(q_values[0])  # Exploit\n",
    "\n",
    "    #Train the DQN using replay buffer samples\n",
    "    def train(self, batch_size=32):\n",
    "        if self.replay_buffer.size() < batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        target_q_values = self.target_network.predict(next_states, verbose=0)\n",
    "        max_next_q = np.max(target_q_values, axis=1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "\n",
    "        q_values = self.q_network.predict(states, verbose=0)\n",
    "        td_errors = targets - q_values[np.arange(batch_size), actions]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            self.update_attention(states[i], td_errors[i])\n",
    "\n",
    "        self.q_network.fit(states, q_values, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    #Update target network with stable weights\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    #Scale beta parameter over time\n",
    "    def anneal_beta(self):\n",
    "        self.beta = min(self.beta + self.delta_beta, self.beta_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f861d-930a-4e6c-9f96-2803fa72cc38",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bd4f7-cd8e-44bb-b4e6-c401bcd9cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"AT-DQN\", name=\"Pong_ATDQN\")\n",
    "\n",
    "env = gym.make('ALE/Pong-v5')\n",
    "state_shape = (84, 84, 4)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = ATDQNAgent(action_size, state_shape)\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Logging Metrics\n",
    "wandb.config.update({\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"batch_size\": batch_size\n",
    "})\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "    state = preprocess_frame(env.reset()[0])\n",
    "    state_stack = np.stack([state] * 4, axis=-1)\n",
    "\n",
    "    total_reward = 0\n",
    "    episode_td_errors = []\n",
    "\n",
    "    for step in range(10000):  # Max steps per episode\n",
    "        action = agent.act(state_stack)\n",
    "        next_frame, reward, done, _, _ = env.step(action)\n",
    "        next_state = preprocess_frame(next_frame)\n",
    "        \n",
    "        next_state_stack = np.append(state_stack[:, :, 1:], np.expand_dims(next_state, axis=-1), axis=-1)\n",
    "        \n",
    "        # Store transition and calculate TD error\n",
    "        td_error = agent.replay_buffer.add((state_stack, action, reward, next_state_stack, done))\n",
    "        episode_td_errors.append(td_error)\n",
    "\n",
    "        agent.train(batch_size)\n",
    "        state_stack = next_state_stack\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    agent.update_target_network()\n",
    "    agent.anneal_beta()\n",
    "    agent.normalize_attention()\n",
    "\n",
    "    # Log episode metrics\n",
    "    wandb.log({\n",
    "        \"Episode\": episode + 1,\n",
    "        \"Total Reward\": total_reward,\n",
    "        \"Mean TD Error\": np.mean(episode_td_errors),\n",
    "        \"Beta\": agent.beta\n",
    "    })\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward}, β = {agent.beta:.4f}\")\n",
    "\n",
    "env.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29ab65-0951-4f6a-92f7-1a751251806f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "atari-r"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
